
Improving the performance of our algorithms can be done in many ways,
but one of the more obvious ones are using parallel computing!

Currently, the most accessible way to run programs in parallel is
using the graphics computation unit (GPU). Modern GPUs are very
powerful, and major manufactures have released software development
kits (SDK) for utilise the GPU for general purpose computation.

One of these SDKs is nVidias CUDA\cit{cuda}. The only thing needed to
use CUDA is a nvidia graphics card that is relatively new (a few years
tops), and the free SDK found at the CUDA website.

As GPUs were developed to render graphics, they are optimized to work
on spatially coherent data. This makes many of our algorithms a
natural target, as we often only need information about neighbouring
data points.

% section?
\subsection{Memory?}

When a algorithm have been implemented using CUDA, there are many ways
to optimize it. Often memory access is the biggest problem.

Because of the architecture of GPUs, CUDA have many different types of
memory. Each with different properties and uses.

\todoPtx{Skriv om threads?}

% code

\begin{lstlisting}
#define GetPhi(phi,x,y,w) phi[x+w*(y)]

void cu_Init() {}


__global__ void reinit(float *phi,float* phi0, float* phin, 
                       unsigned int width, unsigned int height) {
    uint x = __umul24(blockIdx.x, blockDim.x) + threadIdx.x;
    uint y = __umul24(blockIdx.y, blockDim.y) + threadIdx.y;

    if (x > width || y > height)
        return;
    
    float xy = GetPhi(phi,x,y,width);

    float phiXPlus = 0.0f;
    float phiXMinus = 0.0f;
    float phiYPlus = 0.0f;
    float phiYMinus = 0.0f;        	
    if (x != width-1) phiXPlus  = (GetPhi(phi,x+1, y,width) - xy);
    if (x != 0)       phiXMinus = (xy - GetPhi(phi,x-1, y,width));
    
    if (y !=height-1) phiYPlus  = (GetPhi(phi,x, y+1,width) - xy);
    if (y != 0)       phiYMinus = (xy - GetPhi(phi,x, y-1,width));

    /* GetPhi(phin,x,y,width) = phiYPlus; */
    /* return; */


    float dXSquared = 0;
    float dYSquared = 0;
    float a = GetPhi(phi0,x,y,width);
    if (a > 0) {
        // formula 6.3 page 58
        float _max = max(phiXMinus, 0.0f);
        float _min = min(phiXPlus, 0.0f);
        dXSquared = max(_max*_max, _min*_min);
                    
        _max = max(phiYMinus, 0.0f);
        _min = min(phiYPlus, 0.0f);
        dYSquared = max(_max*_max, _min*_min);
    } else {
        // formula 6.4 page 58
        float _max = max(phiXPlus, 0.0f);
        float _min = min(phiXMinus, 0.0f);
        dXSquared = max(_max*_max, _min*_min);
                    
        _max = max(phiYPlus, 0.0f);
        _min = min(phiYMinus, 0.0f);
        dYSquared = max(_max*_max, _min*_min);        				
    }

    float normSquared = dXSquared + dYSquared;           
    float norm = sqrt(normSquared);

    // Using the S(phi) sign formula 7.6 on page 67
    //float sign = phi(x,y) / sqrt(phi(x,y)*phi(x,y) + normSquared);
    float sign = GetPhi(phi0,x,y,width) / 
        sqrt(GetPhi(phi0,x,y,width)*GetPhi(phi0,x,y,width) + 1);
    float t = 0.3; // A stabil CFL condition
    GetPhi(phin,x,y,width) = GetPhi(phi,x,y,width) - sign*(norm - 1)*t;


}

void cu_Reinit(float* data, 
               unsigned int w,
               unsigned int h,
               unsigned int iterations) {
    float* phiData;
    float* phi0Data;
    float* phinData;

    cudaMalloc((void**)&phiData, sizeof(float)*w*h);
    cudaMalloc((void**)&phi0Data, sizeof(float)*w*h);
    cudaMalloc((void**)&phinData, sizeof(float)*w*h);
    cudaMemcpy((void*)phiData,(void*)data, sizeof(float)*w*h,cudaMemcpyHostToDevice);
    cudaMemcpy((void*)phi0Data,(void*)data, sizeof(float)*w*h,cudaMemcpyHostToDevice);
    cudaMemcpy((void*)phinData,(void*)data, sizeof(float)*w*h,cudaMemcpyHostToDevice);


    CHECK_FOR_CUDA_ERROR();

    const dim3 blockSize(32,16,1);
    const dim3 gridSize(w/blockSize.x, h/blockSize.y);

    for (unsigned int i=0;i<iterations;i++) {
        reinit<<<gridSize,blockSize>>>(phiData,phi0Data,phinData,w,h);
        float* tmp = phiData;
        phiData = phinData;
        phinData = tmp;

        cudaThreadSynchronize();
        CHECK_FOR_CUDA_ERROR();
    }

    cudaMemcpy((void*)data,(void*)phiData, sizeof(float)*w*h,cudaMemcpyDeviceToHost);
    CHECK_FOR_CUDA_ERROR();
    cudaFree(phiData);
    cudaFree(phi0Data);
    cudaFree(phinData);
}

\end{lstlisting}

\todoPtx{CUDA coda}
\subsection{Results}

The results in table \ref{tbl:cudaRes} are taken from a system with a
1.8 Ghz Intel Core 2 Duo CPU, 4 GB RAM and a 512MB nVidia GeForce
9600M GT. The time is an average of about 100 iterations of the
algorithm.

\begin{table}[h]
  \centering
  \begin{tabular}{|l|r|r|r|}
    \hline    Algorithm & CPU & GPU & Speedup \\
    % BEGIN RECEIVE ORGTBL numbers
\hline
Reinitialization & 417825 usec & 136675 usec & 2.5987123 \\
- with textures & - & 100006 usec & 3.5515769 \\
\hline
    % END RECEIVE ORGTBL numbers
  \end{tabular}
  \caption{GPU vs. CPU comparison}
  \label{tbl:cudaRes}
\end{table}

The results shows a significant speedup. Using a quite
naive implementation the speedup is easily more than doubled on a inexpensive
consumer graphics card.

\todoPtx{Picture!}


% grep Reinit run1.log | grep CUDA | awk '{sum+=$7} END {print "avg=",sum/NR}'


\begin{comment}
#+ORGTBL: SEND numbers orgtbl-to-latex :splice t :skip 2
|------------------+-------------+-------------+-----------|
|                  | CPU         | GPU         |   Speedup |
|------------------+-------------+-------------+-----------|
| Reinitialization | 417825 usec | 136675 usec | 3.0570697 |
| - with textures  | -           | 100006 usec | 4.1779993 |
|------------------+-------------+-------------+-----------|
#+TBLFM: @2$4=@2$2 / @2$3::@3$4=@2$2 / @3$3
\end{comment}


%%% Local Variables: 
%%% mode: latex
%%% mode: auto-fill
%%% mode: orgtbl
%%% TeX-PDF-mode: t
%%% TeX-master: "../master.tex"
%%% End: 
